{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catching the sentiment\n",
    "\n",
    "Let's see how well deep learning handles text stuff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the IMDB sentiment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a document:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 785, 189, 438, 47, 110, 142, 7, 6, 7475, 120, 4, 236, 378, 7, 153, 19, 87, 108, 141, 17, 1004, 5, 30432, 883, 10789, 23, 8, 4, 136, 13772, 11631, 4, 7475, 43, 1076, 21, 1407, 419, 5, 5202, 120, 91, 682, 189, 2818, 5, 9, 1348, 31, 7, 4, 118, 785, 189, 108, 126, 93, 13772, 16, 540, 324, 23, 6, 364, 352, 21, 14, 9, 93, 56, 18, 11, 230, 53, 771, 74, 31, 34, 4, 2834, 7, 4, 22, 5, 14, 11, 471, 9, 17547, 34, 4, 321, 487, 5, 116, 15, 6584, 4, 22, 9, 6, 2286, 4, 114, 2679, 23, 107, 293, 1008, 1172, 5, 328, 1236, 4, 1375, 109, 9, 6, 132, 773, 14799, 1412, 8, 1172, 18, 7865, 29, 9, 276, 11, 6, 2768, 19, 289, 409, 4, 5341, 2140, 20250, 648, 1430, 10136, 8914, 5, 27, 3000, 1432, 7130, 103, 6, 346, 137, 11, 4, 2768, 295, 36, 7740, 725, 6, 3208, 273, 11, 4, 1513, 15, 1367, 35, 154, 14040, 103, 19100, 173, 7, 12, 36, 515, 3547, 94, 2547, 1722, 5, 3547, 36, 203, 30, 502, 8, 361, 12, 8, 989, 143, 4, 1172, 3404, 10, 10, 328, 1236, 9, 6, 55, 221, 2989, 5, 146, 165, 179, 770, 15, 50, 713, 53, 108, 448, 23, 12, 17, 225, 38, 76, 4397, 18, 183, 8, 81, 19, 12, 45, 1257, 8, 135, 15, 13772, 166, 4, 118, 7, 45, 12831, 17, 466, 45, 24410, 4, 22, 115, 165, 764, 6075, 5, 1030, 8, 2973, 73, 469, 167, 2127, 18281, 1568, 6, 87, 841, 18, 4, 22, 4, 192, 15, 91, 7, 12, 304, 273, 1004, 4, 1375, 1172, 2768, 12356, 15, 4, 22, 764, 55, 5773, 5, 14, 4233, 7444, 4, 1375, 326, 7, 4, 4760, 1786, 8, 361, 1236, 8, 989, 46, 7, 4, 2768, 45, 55, 776, 8, 79, 496, 98, 45, 400, 301, 15, 4, 1859, 9, 4, 155, 15, 66, 21885, 84, 5, 14, 22, 1534, 15, 17, 4, 167, 12356, 15, 75, 70, 115, 66, 30, 252, 7, 618, 51, 9, 2161, 4, 3130, 5, 14, 1525, 8, 6584, 15, 13772, 165, 127, 1921, 8, 30, 179, 2532, 4, 22, 9, 906, 18, 6, 176, 7, 1007, 1005, 4, 1375, 114, 4, 105, 26, 32, 55, 221, 11, 68, 205, 96, 5, 4, 192, 15, 4, 274, 410, 220, 304, 23, 94, 205, 109, 9, 55, 73, 224, 259, 3786, 15, 4, 22, 528, 1645, 34, 4, 130, 528, 30, 685, 345, 17, 4, 277, 199, 166, 281, 5, 1030, 8, 30, 179, 4442, 444, 13772, 9, 6, 371, 87, 189, 22, 5, 31, 7, 4, 118, 7, 4, 2068, 545, 1178, 829]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite what we expected... Keras has already replaced each word with its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tensorflow and keras do not support dynamic graphs (yet?), we have to pad the documents (and possibly truncate the longer documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words -> consider only the top 10000 most frequent words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    1  785  189  438   47  110\n",
      "  142    7    6 7475  120    4  236  378    7  153   19   87  108  141\n",
      "   17 1004    5    2  883    2   23    8    4  136    2    2    4 7475\n",
      "   43 1076   21 1407  419    5 5202  120   91  682  189 2818    5    9\n",
      " 1348   31    7    4  118  785  189  108  126   93    2   16  540  324\n",
      "   23    6  364  352   21   14    9   93   56   18   11  230   53  771\n",
      "   74   31   34    4 2834    7    4   22    5   14   11  471    9    2\n",
      "   34    4  321  487    5  116   15 6584    4   22    9    6 2286    4\n",
      "  114 2679   23  107  293 1008 1172    5  328 1236    4 1375  109    9\n",
      "    6  132  773    2 1412    8 1172   18 7865   29    9  276   11    6\n",
      " 2768   19  289  409    4 5341 2140    2  648 1430    2 8914    5   27\n",
      " 3000 1432 7130  103    6  346  137   11    4 2768  295   36 7740  725\n",
      "    6 3208  273   11    4 1513   15 1367   35  154    2  103    2  173\n",
      "    7   12   36  515 3547   94 2547 1722    5 3547   36  203   30  502\n",
      "    8  361   12    8  989  143    4 1172 3404   10   10  328 1236    9\n",
      "    6   55  221 2989    5  146  165  179  770   15   50  713   53  108\n",
      "  448   23   12   17  225   38   76 4397   18  183    8   81   19   12\n",
      "   45 1257    8  135   15    2  166    4  118    7   45    2   17  466\n",
      "   45    2    4   22  115  165  764 6075    5 1030    8 2973   73  469\n",
      "  167 2127    2 1568    6   87  841   18    4   22    4  192   15   91\n",
      "    7   12  304  273 1004    4 1375 1172 2768    2   15    4   22  764\n",
      "   55 5773    5   14 4233 7444    4 1375  326    7    4 4760 1786    8\n",
      "  361 1236    8  989   46    7    4 2768   45   55  776    8   79  496\n",
      "   98   45  400  301   15    4 1859    9    4  155   15   66    2   84\n",
      "    5   14   22 1534   15   17    4  167    2   15   75   70  115   66\n",
      "   30  252    7  618   51    9 2161    4 3130    5   14 1525    8 6584\n",
      "   15    2  165  127 1921    8   30  179 2532    4   22    9  906   18\n",
      "    6  176    7 1007 1005    4 1375  114    4  105   26   32   55  221\n",
      "   11   68  205   96    5    4  192   15    4  274  410  220  304   23\n",
      "   94  205  109    9   55   73  224  259 3786   15    4   22  528 1645\n",
      "   34    4  130  528   30  685  345   17    4  277  199  166  281    5\n",
      " 1030    8   30  179 4442  444    2    9    6  371   87  189   22    5\n",
      "   31    7    4  118    7    4 2068  545 1178  829]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are ready to extract the sentiment from the documents!!! We will use a simple word embedding-based MLP for the classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               8000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 8,321,001\n",
      "Trainable params: 8,321,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, AveragePooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# Number of unique words, embedding dimension, number of words per document\n",
    "model.add(Embedding(10000, 32, input_length=500))\n",
    "# Just flatten the embedding vector (does not takes into account the padding!)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.5301 - acc: 0.7013 - val_loss: 0.3034 - val_acc: 0.8700\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.1831 - acc: 0.9297 - val_loss: 0.3044 - val_acc: 0.8736\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 3s 127us/step - loss: 0.0471 - acc: 0.9871 - val_loss: 0.4082 - val_acc: 0.8675\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 3s 122us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.5182 - val_acc: 0.8626\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.5605 - val_acc: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43abe27470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, using just the mean embedding vector works equally good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               16500     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 337,001\n",
      "Trainable params: 337,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=500))\n",
    "\n",
    "# Calculate the mean embedding\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters are greatly reduced. Let's examine the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1s 46us/step - loss: 0.5729 - acc: 0.7065 - val_loss: 0.3824 - val_acc: 0.8363\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1s 40us/step - loss: 0.2855 - acc: 0.8906 - val_loss: 0.2881 - val_acc: 0.8850\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1s 39us/step - loss: 0.2179 - acc: 0.9180 - val_loss: 0.3025 - val_acc: 0.8737\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1s 40us/step - loss: 0.1795 - acc: 0.9358 - val_loss: 0.2882 - val_acc: 0.8861\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1s 39us/step - loss: 0.1535 - acc: 0.9464 - val_loss: 0.2974 - val_acc: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f439d0e10f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually works better (this is expected since the flattening operator keeps too much temporal information that the used MLP cannot use). Also, let's try to ignore the padded words (masking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               16500     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 337,001\n",
      "Trainable params: 337,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 2s 62us/step - loss: 0.5914 - acc: 0.6866 - val_loss: 0.3840 - val_acc: 0.8456\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1s 59us/step - loss: 0.2945 - acc: 0.8850 - val_loss: 0.2924 - val_acc: 0.8838\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.2209 - acc: 0.9181 - val_loss: 0.2836 - val_acc: 0.8851\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.1843 - acc: 0.9323 - val_loss: 0.2907 - val_acc: 0.8835\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.1584 - acc: 0.9439 - val_loss: 0.2954 - val_acc: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f439ee98940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Masking\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(500,)))\n",
    "model.add(Embedding(10000, 32, input_length=500))\n",
    "\n",
    "# Calculate the mean embedding\n",
    "model.add(AveragePooling1D(pool_size=500))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking does not seem to significantly impact the performance of the model. We can also, use a CNN for text classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 32)           3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               16500     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 340,105\n",
      "Trainable params: 340,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s 105us/step - loss: 0.5502 - acc: 0.7030 - val_loss: 0.3618 - val_acc: 0.8398\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.3046 - acc: 0.8731 - val_loss: 0.2994 - val_acc: 0.8723\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.2221 - acc: 0.9110 - val_loss: 0.2835 - val_acc: 0.8822\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.1645 - acc: 0.9363 - val_loss: 0.3112 - val_acc: 0.8741\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.1280 - acc: 0.9534 - val_loss: 0.3284 - val_acc: 0.8761\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.0982 - acc: 0.9632 - val_loss: 0.3532 - val_acc: 0.8747\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.0833 - acc: 0.9689 - val_loss: 0.4009 - val_acc: 0.8690\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 78us/step - loss: 0.0664 - acc: 0.9762 - val_loss: 0.3952 - val_acc: 0.8769\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s 78us/step - loss: 0.0567 - acc: 0.9788 - val_loss: 0.4329 - val_acc: 0.8742\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 76us/step - loss: 0.0491 - acc: 0.9825 - val_loss: 0.4441 - val_acc: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43a36acd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, GlobalAveragePooling1D, GlobalMaxPool1D, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(500,)))\n",
    "model.add(Embedding(10000, 32, input_length=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=32, kernel_size=3))\n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent models, such as LSTMs and GRUs, can be also very easily used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_3 (Masking)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               82944     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 467,945\n",
      "Trainable params: 467,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 10s 419us/step - loss: 0.5265 - acc: 0.7500 - val_loss: 0.3319 - val_acc: 0.8620\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 10s 405us/step - loss: 0.2718 - acc: 0.8924 - val_loss: 0.3508 - val_acc: 0.8578\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 10s 402us/step - loss: 0.2057 - acc: 0.9224 - val_loss: 0.3416 - val_acc: 0.8618\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 10s 401us/step - loss: 0.1905 - acc: 0.9300 - val_loss: 0.4332 - val_acc: 0.8497\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 10s 409us/step - loss: 0.1814 - acc: 0.9320 - val_loss: 0.3412 - val_acc: 0.8652\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 10s 404us/step - loss: 0.1463 - acc: 0.9477 - val_loss: 0.3861 - val_acc: 0.8524\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 10s 409us/step - loss: 0.1255 - acc: 0.9551 - val_loss: 0.3935 - val_acc: 0.8601\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 10s 405us/step - loss: 0.1134 - acc: 0.9588 - val_loss: 0.4241 - val_acc: 0.8641\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 10s 404us/step - loss: 0.1005 - acc: 0.9644 - val_loss: 0.4950 - val_acc: 0.8632\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 10s 405us/step - loss: 0.0847 - acc: 0.9708 - val_loss: 0.4428 - val_acc: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f439c545fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(500,)))\n",
    "model.add(Embedding(10000, 32, input_length=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some hyper-parameter tunning you might be able to further increase the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
